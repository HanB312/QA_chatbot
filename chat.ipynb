{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f6db16-0a9f-4233-9d31-aaceebefdb52",
   "metadata": {},
   "source": [
    "## 논문 chat bot 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfae3e8-e692-404e-925c-636f82fef8d8",
   "metadata": {},
   "source": [
    "문서 기반 질의응답 시스템  \n",
    "LLM (HuggingFace 모델)  \n",
    "RAG (정보 검색형 Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f232a8-df54-4b4f-b6d7-5cc20d657f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 설치\n",
    "!pip install langchain faiss-cpu transformers torch sentencepiece\n",
    "!pip install sentence-transformers\n",
    "!pip install -U langchain-community\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8c473b-48fa-4a16-adf3-3b408f47d531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlgks\\anaconda3\\envs\\langchat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e73c60f-94a3-48a7-adf1-ce9b036a9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlgks\\AppData\\Local\\Temp\\ipykernel_16804\\294971210.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# PDF 불러오기\n",
    "loader = PyPDFLoader(\"chat_doc/DAEM-ERC.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 텍스트 청크로 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 임베딩 + 벡터스토어 생성\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf27dea-c4c5-4345-9797-26ea8a4e2976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 로컬 LLM 불러오기 (HuggingFace 모델)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_new_tokens=256,\n",
    "                temperature=0.4,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c0e530-e00c-4f54-b297-4b97f7839452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질의응답 함수\n",
    "def tinyllama_chat_prompt(context, question):\n",
    "    # Llama 계열 chat 템플릿을 tokenizer가 자동 생성\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"너는 문서 기반 한국어 QA 모델이야. 주어진 문맥에서만 근거를 찾아 한국어 한두 문장으로 간결히 답해. 문맥에 없으면 '모르겠습니다.'라고 말해.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"다음 문맥을 근거로 질문에 답해.\\n\\n[문맥]\\n{context}\\n\\n[질문]\\n{question}\\n\\n규칙: 불필요한 서론 금지, 근거 없는 추측 금지, 한두 문장만 출력.\"}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def ask(question, k=2):\n",
    "    # 1) 검색\n",
    "    rel = vectorstore.similarity_search(question, k=k)\n",
    "    context = \"\\n\\n---\\n\\n\".join(d.page_content for d in rel)\n",
    "\n",
    "    # 2) 프롬프트 생성(템플릿)\n",
    "    prompt = tinyllama_chat_prompt(context, question)\n",
    "\n",
    "    # 3) 생성\n",
    "    out = pipe(prompt, return_full_text=False, max_new_tokens=256)[0][\"generated_text\"]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0701842e-ecec-4571-b545-75678eb1909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study aims to improve the performance of sentiment analysis models by using data augmentation techniques such as over-sampling for text, audio, and body sensor data. The researchers use different types of data (text, audio, and body sensor data) but apply the same over-sampling technique to all three datasets. They also propose a model that uses both text, audio, and body sensor data for classification, with each dataset having its own set of features. The performance of the model is evaluated using various metrics, including accuracy, precision, recall, F1 score, and weighted F1 score. The results show that the proposed model outperforms other models in terms of accuracy, precision, and F1 score, with a significant improvement in the case of the body sensor data. Additionally, the authors note that the use of over-sampling techniques can help alleviate the problem of class imbalance in the data and improve the overall performance of the model.\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "print(ask(\"사용한 성능지표가 뭐야?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412357a2-7717-4ac7-a065-a91b69e582f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff985-cbea-4ab8-a28f-bf278135cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
